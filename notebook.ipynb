{"nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["\n<h2 id=\"Introduction\">Introduction<a class=\"anchor-link\" href=\"#Introduction\">\u00b6</a></h2><p>In this notebook I'll introduce my results in a super resolution exercise.\nOur goal is to reverse a given picture in a low resultion into higher resultion.</p>\n<p>Some of the learning methods this exercise will introduce are <em>residual blocks</em>, <em>transfer learning</em>, <em>dilated convolution</em> etc.</p>\n<p>The data set we used is from the PASCAL Visual Object Classes Challenge 2007 of kaggle.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\n%pip install --user opencv-python\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n%pip install git https://github.com/titu1994/keras-efficientnets.git\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nfrom PIL import Image\nimport glob\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import *\nimport numpy as np\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nimport keras.backend as K\nimport tensorflow as tf\nimport gc\nimport cv2\nimport pandas as pd\nfrom keras.models import model_from_json\nfrom os import listdir\nfrom os.path import join\nimport os\nclass LeakyRELU(LeakyReLU):\n    def __init__(self, **kwargs):\n        self.__name__ = \"LeakyRELU\"\n        super(LeakyRELU, self).__init__(**kwargs)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Data-Pre-Process\">Data Pre-Process<a class=\"anchor-link\" href=\"#Data-Pre-Process\">\u00b6</a></h3><p>Our data consists of 5011 in different shapes. We are interested in creating data set &lt;X,Y&gt; such that X shape is <code>(72,72,3)</code>, <code>Y[0]</code> shape is  <code>(144,144,3)</code> and <code>Y[1]</code> shape is  <code>(288,288,3)</code>. In order to make our preprocess comfortable and efficient we'll use the lazy data structure <em>Generator</em></p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\n# Some sampling from the data shapes\nimages = []\nfor im in listdir('first_20'):\n    images.append(cv2.imread('first_20/{}'.format(im)))\nprint(images[0].shape,images[1].shape, images[10].shape)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dir = 'VOCdevkit_v2/VOC2007/JPEGImages/train'\nval_dir = 'VOCdevkit_v2/VOC2007/JPEGImages/val'\ninp_shape = (72,72,3)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n#Our memmory friendly generator\nclass MyGen(object):\n    def __init__(self, pti):\n        self._pti = pti\n    def __iter__(self):\n        i=0\n        x = []\n        y_144 = []\n        y_288 = []\n        for file in os.listdir(self._pti):\n            im = cv2.imread(os.path.join(self._pti, file))\n            if im is not None:\n                im = im/255. ##normalizing the picture\n                x.append(cv2.resize(im, (72,72,)))\n                y_144.append(cv2.resize(im, (144,144,)))\n                y_288.append(cv2.resize(im, (288,288,)))\n                i+=1 \n            if i % 32 ==0:\n                yield (np.array(x), [np.array(y_144),np.array(y_288)])\n                x = []\n                y_144 = []\n                y_288 = []\n\n                \n                \nclass TrainGen(MyGen):\n    def __init__(self):\n        super(TrainGen, self).__init__(train_dir)\nclass ValGen(MyGen):\n    def __init__(self):\n        super(ValGen, self).__init__(val_dir)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nimages = list(\n    map(\n        lambda x: x/255,\n        images\n    )\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nim_72 = list(\n    map(\n        lambda x: cv2.resize(x,(72,72)),\n        images\n    )\n)\nim_144 = list(\n    map(\n        lambda x: cv2.resize(x,(144,144)),\n        images\n    )\n)\nim_288 = list(\n    map(\n        lambda x: cv2.resize(x,(288,288)),\n        images\n    )\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n#Checking our validation data\nval_batch = next(ValGen().__iter__())\n\n\nfor some_index in [0, 8 , 30]:\n    _, ax = plt.subplots(1,3, figsize =(20,5))\n    ax[0].imshow(val_batch[0][some_index])\n    ax[0].set_title(\"77*77\")\n    ax[1].imshow(val_batch[1][0][some_index])\n    ax[1].set_title(\"144*144\")\n    ax[2].imshow(val_batch[1][1][some_index])\n    ax[2].set_title(\"288*288\")\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"First-Model\">First Model<a class=\"anchor-link\" href=\"#First-Model\">\u00b6</a></h3><p>Our first model will be the simplest. We'll use two conolutional layers, keeping the our original horizontal and vertical dimension. We then upsample once for the 144*144 output and twice for the 288*288 output.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef train_model(model):\n    return model.fit_generator(\n        TrainGen().__iter__(),\n        steps_per_epoch=10,\n        validation_data = ValGen().__iter__(),\n        validation_steps = 1,\n        epochs = 5\n    )\n        \n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef show_learning_history(history):\n    fig, ax = plt.subplots(1,1,figsize=(12,4))\n    # Plot training & validation loss values\n    ax.plot(history.history['loss'])\n    ax.plot(history.history['val_loss'])\n    ax.set_title('Model loss')\n    ax.set_ylabel('Loss')\n    ax.set_xlabel('Epoch')\n    ax.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef examine_model(idx, x, y_144,y_288, model):\n    f_orig, axarr_orig = plt.subplots(1,1, figsize = (5,5), )\n    axarr_orig.imshow(\n        x[idx]\n    )\n    axarr_orig.set_title(\"orig 72*72\")\n    \n    f, axarr = plt.subplots(2,2, figsize = (10,10))\n    \n    axarr[0][0].imshow(\n        y_144[idx]\n    )\n    axarr[0][0].set_title(\"orig 144*144\")\n    axarr[0][1].imshow(\n        model.predict(np.expand_dims(x[idx], axis = 0))[0][0]\n    )\n    axarr[0][1].set_title(\"pred 144*144\")\n    \n    axarr[1][0].imshow(\n        y_288[idx]\n    )\n    axarr[1][0].set_title(\"orig 288*288\")\n    axarr[1][1].imshow(\n        model.predict(np.expand_dims(x[idx], axis = 0))[1][0]\n    )\n    axarr[1][1].set_title(\"pred 288*288\")\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef create_simple_model():\n    inp = Input(shape=(72,72,3))\n    x = Conv2D(64, 3, padding = 'same')(inp)\n    x = Conv2D(64, 3,padding = 'same')(x)\n    x_144 = UpSampling2D(size = (2,2))(x)\n    x_288 = UpSampling2D(size = (4,4))(x)\n    x_144 =Conv2D(3 ,1)(x_144)\n    x_288 =Conv2D(3 ,1)(x_288)\n    return Model(inp, [x_144,x_288])\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nmodel = create_simple_model()\nmodel.compile(optimizer = 'adam', loss = 'mse')\nmodel.summary()\nh = train_model(model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Residual-blocks\">Residual blocks<a class=\"anchor-link\" href=\"#Residual-blocks\">\u00b6</a></h3><p>Our next strategy is going to be residual blocks, with an hope that our model will learn if some block of layers improves our results.\nOur residual block can be visualized like this:</p>\n<p><img src=\"markdown/res_block.png\"/></p>\n<p>And our whole model, shall look as follow:</p>\n<p><img src=\"markdown/res_block_model.png\"/></p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef res_block():\n    inp = Input(shape = (None, None, 64))\n    x = Conv2D(64, 3, activation=LeakyRELU(alpha = 0.2), padding = 'same')(inp)\n    x = Conv2D(64, 3, activation=LeakyRELU(alpha = 0.2), padding = 'same')(x)\n    x = add([x, inp])\n    x = Activation(LeakyRELU(alpha = 0.2))(x)\n    return Model(inp, x)\n    \n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef create_res_block_model(weights = None):    \n    inp = Input(shape = (72,72,3))\n    x = Conv2D(64,1)(inp)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x_144 = UpSampling2D(size = (2,2))(x)\n    x_288 = UpSampling2D(size = (4,4))(x)\n    x_144 =Conv2D(3 ,1)(x_144)\n    x_288 =Conv2D(3 ,1)(x_288)\n    model = Model(inp, [x_144,x_288])\n    if weights is None:\n        return model\n    else:\n        model.load_weights(weights)\n        return model\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nres_block_model = create_res_block_model()\nres_block_model.compile(optimizer = 'adam', loss = 'mse')\nres_block_model.summary()\nh = train_model(res_block_model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = res_block_model\n)\nexamine_model(\n    idx = 1,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = res_block_model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Dilated-residual-blocks\">Dilated residual blocks<a class=\"anchor-link\" href=\"#Dilated-residual-blocks\">\u00b6</a></h3><p>We are tying this model with a sense that maybe it is \"informable\" to infere conclusion about a pixel taking into account not only it's adjecent neighbours but some of his neighbours of his neighbours. We will add 3 residual blocks withs 1 2 and four dialation rate.\n<img src=\"markdown/dilated_res.png\"/></p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef res_dialted_block():\n    inp = Input(shape=(None,None,32))\n    d1 = Conv2D(32, 3, padding='same', activation='relu', dilation_rate=1)(inp)\n    d2 = Conv2D(32, 3, padding='same', activation='relu', dilation_rate=2)(inp)\n    d4 = Conv2D(32, 3, padding='same', activation='relu', dilation_rate=4)(inp)\n    x = Concatenate()([d1, d2, d4])\n    x = Conv2D(32,1)(x)\n    x = Add()([x , inp])\n    x = Activation(LeakyRELU())(x)\n    return Model(inp,x)\n\ndef create_dilated_model():\n    inp = Input(shape = (72,72,3))\n    x = Conv2D(32,1 , padding='same', activation='relu')(inp)\n    x = res_dialted_block()(x)\n    x = res_dialted_block()(x)\n    x = UpSampling2D()(x)\n    x_144 = Conv2D(3,(1,1),activation='relu')(x)\n    x_288 = res_dialted_block()(x)\n    x_288 = UpSampling2D()(x_288)\n    x_288 = Conv2D(3,(1,1),activation='relu')(x_288)\n    return Model(inp, [x_144,x_288])\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nmodel = create_dilated_model()\nmodel.compile(optimizer = 'adam', loss = 'mse')\nmodel.summary()\nh = train_model(model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndilated_model = model\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = dilated_model\n)\nexamine_model(\n    idx = 1,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = dilated_model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>We can see that in this  model the model didn't overfit quickly to the training set. So we we'll give it some more epoches.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nh = train_model(dilated_model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = dilated_model\n)\nexamine_model(\n    idx = 1,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = dilated_model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Using-pretrained-model\">Using pretrained model<a class=\"anchor-link\" href=\"#Using-pretrained-model\">\u00b6</a></h3><p>We will use VGG16 as a feature extractor to our model</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef create_vgg_model():\n    VGG16(include_top =False).get_layer(\"block1_conv2\")\n    inp = Input(shape = (72,72,3))\n    x = Conv2D(kernel_size = 1,filters = 64)(inp)\n    x = VGG16(include_top =False).get_layer(\"block1_conv2\")(x)\n    x = concatenate([inp,x])\n    x = Conv2D(64, 3, padding = 'same')(inp)\n    x = Conv2D(64, 3,padding = 'same')(x)\n    x_144 = UpSampling2D(size = (2,2))(x)\n    x_288 = UpSampling2D(size = (4,4))(x)\n    x_144 =Conv2D(3 ,1)(x_144)\n    x_288 =Conv2D(3 ,1)(x_288)\n    model = Model(inp, [x_144,x_288])\n    return model\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\npre_trained_model = create_vgg_model()\npre_trained_model.compile(optimizer = 'adam', loss = 'mse')\npre_trained_model.summary()\nh = train_model(pre_trained_model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = pre_trained_model\n)\nexamine_model(\n    idx = 1,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = pre_trained_model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nval_batch[1][0][1].shape\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_model(pre_trained_model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\npre_trained_model.save_weights(join(\"models\", \"vgg.h5\"))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Depth-to-space\">Depth to space<a class=\"anchor-link\" href=\"#Depth-to-space\">\u00b6</a></h3><p>In this approach we're trying to use the number of features maps in one of our layers, hoping that it will reflect in a good way in the spacial dimension</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef create_d2pace_model():\n    inp = Input(shape = (72,72,3))\n    x = Conv2D(64,1, activation=LeakyRELU(alpha = 0.2))(inp)\n    x = Activation(LeakyRELU(alpha = 0.2))(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = res_block()(x)\n    x = Activation(LeakyRELU(alpha = 0.2))(x)\n    x_144 = Lambda(lambda x: tf.depth_to_space(input=x,block_size=2))(x)\n    x_144 = Activation(LeakyRELU(alpha = 0.2))(x_144)\n    x_288 =Lambda(lambda x: tf.depth_to_space(input=x,block_size=2,))(x_144)\n    x_288 = Activation(LeakyRELU(alpha = 0.2))(x_288)\n    x_144 =Conv2D(3 ,1, activation=LeakyRELU(alpha = 0.2))(x_144)\n    x_288 = Conv2D(3 ,1, activation=LeakyRELU(alpha = 0.2))(x_288)\n    return Model(inp, [x_144,x_288])\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nd2space_model = create_d2pace_model()\nd2space_model.compile(optimizer = 'adam', loss = 'mse')\nd2space_model.summary()\nh = train_model(d2space_model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = d2space_model\n)\nexamine_model(\n    idx = 1,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = d2space_model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\npre_trained_model.save_weights(join(\"models\", \"d2space.h5\"))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"More-residual-blocks\">More residual blocks<a class=\"anchor-link\" href=\"#More-residual-blocks\">\u00b6</a></h3><p>In this model we tried to reduce the depth of our net and keep it complex. We did it by making residual blocks to layers the precede the up sampling.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef create_more_res_model(weights = None):\n    inp = Input(shape = (72,72,3))\n    x = Conv2D(64,1)(inp)\n\n    res_1 = res_block()(x)\n    x = MaxPool2D()(res_1)\n\n    res_2 = res_block()(x)\n    x = MaxPool2D()(res_2)\n\n    x = UpSampling2D()(x)\n\n    x = add([x, res_2])\n    x = res_block()(x)\n\n    x = UpSampling2D()(x)\n    x = add([x, res_1])\n\n    x_144 = UpSampling2D()(x)\n    x_288 = UpSampling2D(size = (4,4))(x)\n    x_144 =Conv2D(3 ,1)(x_144)\n    x_288 =Conv2D(3 ,1)(x_288)\n    model = Model(inp, [x_144,x_288])\n    if weights is None:\n        return model\n    else:\n        model.load_weights(weights)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nmore_res_model = create_more_res_model()\nmore_res_model.compile(optimizer = 'adam', loss = 'mse')\nmore_res_model.summary()\nh = train_model(more_res_model)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nshow_learning_history(h)\nexamine_model(\n    idx = 0,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = more_res_model\n)\nexamine_model(\n    idx = 1,\n    x = val_batch[0],\n    y_144 = val_batch[1][0],\n    y_288 = val_batch[1][1],\n    model = more_res_model\n)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\npre_trained_model.save_weights(join(\"models\", \"more_res.h5\"))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Summary\">Summary<a class=\"anchor-link\" href=\"#Summary\">\u00b6</a></h3><p>We tried few differnt approches. It seems as the dilated convolution approach was the best considering it's not bad output and low over fitting.</p>\n"], "cell_type": "markdown"}], "metadata": {}}